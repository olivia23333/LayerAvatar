
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>

<link rel="stylesheet" type="text/css" href="style.css" />
  

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<!-- <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'> -->
<!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,300,300italic,400italic,600,600italic,700,700italic,800,800italic' rel='stylesheet' type='text/css'> -->
<link href='https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro' rel='stylesheet' type='text/css'>
  
<head>
    <title>Disentangled Clothed Avatar Generation with Layered Representation</title>
    <meta property="og:description" content="Disentangled Clothed Avatar Generation with Layered Representation"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>

</head>


 <body>
<div class="container">
    <div class="paper-title">
      <h1>Disentangled Clothed Avatar Generation with Layered Representation</h1>
    </div>
  

    
    <div id="authors">
        <div class="author-row">
            <div class="col-4 text-center">Weitian Zhang<sup>1</sup></div>
            <div class="col-4 text-center">Sijing Wu<sup>1</sup></div>
            <div class="col-4 text-center">Manwen Liao<sup>2</sup></div>
            <div class="col-4 text-center"><a href="https://daodaofr.github.io/">Yichao Yan</a><sup>1</sup></div>
        </div>

<!--         <div class="affil-row">
            <div class="col-2 text-center"><sup>1</sup><a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a></div>
            <div class="col-2 text-center"><sup>2</sup>Alibaba</a></div>
        </div> -->
        <table align=center width="50%">
            <tr>
            <td colspan="1">
                <sup>1</sup>Shanghai Jiao Tong University
            </td>
            <td colspan="1">
                <sup>2</sup>The University of Hong Kong
            </td>
            </tr>
        </table>
      
      <!-- <h4>(ACMMM 2024 submmited)</h4> -->

<!--         <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_GANHead_Towards_Generative_Animatable_Neural_Head_Avatars_CVPR_2023_paper.pdf">
              <image src="assets/paper_icon.png" height="55px">
              <h5><strong>Paper</strong></h5>
            </a>
            <a class="supp-btn" href="https://github.com/wsj-sjtu/GANHead">
              <image src="assets/github_icon.png" height="55px">
              <h5><strong>Code</strong></h5>
            </a>
            <a class="supp-btn" href="assets/bib.txt">
                <span class="material-icons"> description </span>
                  BibTeX
            </a>
        </div></div> -->

        <table align=center width="50%">
            <tr>
            <td colspan="1">
                <a href="http://arxiv.org/abs/2501.04631" target="_blank">
                    <div style="text-align: center;">
                    <image src="assets/paper_icon.png" height="55px" style="display: block; margin: 0 auto; margin-bottom: 10px;">
                    <h5><strong>Arxiv</strong></h5>
                </div>
                </a>
            </td>
            <!-- <td colspan="1">
                <a href="https://github.com/wsj-sjtu/GANHead" target="_blank">
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<image src="assets/github_icon.png" height="55px">
                    <h5><strong>Code</strong></h5>
                </a>
            </td> -->
            <td colspan="1">
                <a href="https://github.com/olivia23333/LayerAvatar" target="_blank">
                    <div style="text-align: center;">
                    <image src="assets/github_icon.png" height="55px" style="display: block; margin: 0 auto; margin-bottom: 10px;">
                    <h5><strong>Code</strong></h5>
                </div>
                </a>
            </td>
            <!-- <td colspan="1">
                <a href="" target="_blank">
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp<image src="assets/youtube_icon.png" height="55px">
                    <h5><strong>Video</strong></h5>
                </a>
            </td> -->
            </tr>
         </table>
      
    </div>

    <section id="teaser">
        <a href="assets/teaser_new.pdf">
            <img width="100%" src="assets/teaser_new.pdf">
        </a>
        <p align=center class="caption"> We propose <strong>LayerAvatar</strong> to efficiently generate diverse clothed avatars with components fully disentangled. The generated avatars 
            can be animated and synthesized in novel views. They can also be decomposed into body, hair, and clothes for component transfer.
        </p>
    </section>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr/>

        <p> Clothed avatar generation has wide applications in virtual and augmented reality, filmmaking, and more. Previous methods have achieved 
            success in generating diverse digital avatars, however, generating avatars with disentangled components (\eg, body, hair, and clothes) 
            has long been a challenge. 
            In this paper, we propose <strong>LayerAvatar</strong>, the first feed-forward diffusion-based method for generating component-disentangled clothed avatars. 
            To achieve this, we first propose a layered UV feature plane representation, where components are distributed in different layers of the 
            Gaussian-based UV feature plane with corresponding semantic labels. This representation supports high-resolution and real-time rendering, 
            as well as expressive animation including controllable gestures and facial expressions. Moreover, we propose a semantic-aware compositional 
            rendering strategy to facilitate the full disentanglement of each component. Based on the well-designed representation, we train a single-stage 
            diffusion model and introduce constrain terms to address the severe occlusion problem of the innermost human body layer. Extensive experiments 
            demonstrate the impressive performances of our method in generating disentangled clothed avatars, and we further explore its applications in component transfer.
            </p>
    </section>
    
    
    <section id="Demo Video">
        <h2>Full Video</h2>
        <hr/>
        <figure style="width: 100%;">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/video_page_new.mp4" type="video/mp4">
            </video>
            <!-- <p class="caption">The demo video shows avatar generation and animation results(controled by SMPL), followed by the latent code interpolation results. -->
            <!-- </p> -->
        </figure>
        <hr/>
    </section>
   
   
    <section id="method">
        <h2>Method Overview</h2>
        <hr/>
        <figure style="width: 100%;">
            <a href="assets/method3.pdf">
                <img width="100%" src="assets/method3.pdf">
            </a>
            <p>
                LayerAvatar learns a feed-forward diffusion model to generate clothed avatars with each component disentangled. The clothed avatars are represented as layered UV feature plane where components are represented separately. 
                After decoding the feature plane into attribute maps, we can extract 3D Gaussians from them through SMPL-X-based templates. Generated clothed avatars are then transformed into targeted pose space for further supervision. 
                Reconstruction loss and constrain loss are both utilized to facilitate the disentanglement and handle the severe occlusion of human body layer.
            </p>
        </figure>
   
    </section>
   

    <section id="Results">
        <!-- <h2>Application</h2> -->
        <!-- <hr/>
        <h3>Face Reenactment</h3>
        <figure style="width: 80%;">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/reenactment.mp4" type="video/mp4">
            </video>
        </figure>
        <p class="caption"> We estimate the FLAME parameters of the sourse video, and use the estimated pose and expression parameters to animate the generated head avatars. 
                                The generated avatars can fully reproduce FLAME's poses and expressions.
        </p>
        <br/>
        <hr/> -->
      
        <h2>Random Generation</h2>
        <figure style="width: 80%;">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/random_gen.mp4" type="video/mp4">
            </video>
        </figure>
        <p class="caption"> Our method can generate fully disentangled avatars wearing diverse clothes. The generated digital avatars exhibit details such as distinct fingers and cloth wrinkles.
        </p>

        <h2>Novel Pose Animation</h2>
        <figure style="width: 80%;">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/animation.mp4" type="video/mp4">
            </video>
        </figure>
        <p class="caption"> We demonstrate the novel pose animation ability using pose sequences in AMASS and X-Avatar. Our method can also handel vivid gesture and facial expression control.
        </p>

        <h2>Component Transfer</h2>
        <figure style="width: 80%;">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/transfer.mp4" type="video/mp4">
            </video>
        </figure>
        <p class="caption"> We exhibit component transfer application of our method. With disentangled components, we can directly transfer hairstyles, clothes, and shoes to enable customization of digital avatars.
        </p>
    </section>

    <!-- <section id="Other Results">
        <h2>Other Results</h2>
        <hr/>
        <table align=center width="100%">
            <tr>
            <td colspan="4">
                <h3>Sample Latent Codes</h3>
            </td>
            <td colspan="5">
                <h3>Raw Scan Fitting</h3>
            </td>
            </tr>
          
            <tr>
            <td colspan="2">
                <center>
                <video width=150px controls muted loop autoplay>
                    <source src="assets/shape_code.mp4" type="video/mp4">
                </video>
            </center>
            </td>
            <td colspan="2">
                <center>
                <video width=150px controls muted loop autoplay>
                    <source src="assets/detail_code.mp4" type="video/mp4">
                </video>
                </center>
            </td>
            <td colspan="5">
                <center>
                <a href="assets/multiface_fitting.png">
                    <img width=640px src="assets/multiface_fitting.png">
                </a>
                </center>
            </td>
            </tr>
        </table>
        <p class="caption"> We also train our model on a subset of Multiface datase. Since Multiface datase has less detail and noise in the hair region, 
                            our model can learn more details of the facial region and achieve better fitting results.
        </p>
        <hr/>
    </section> -->

    <section class="section" id="bibtex">
      <h2>Citation</h2>
      <div style="overflow-x:auto;">
<!--         <hr/> -->
        <pre style="background-color: #e9eeef;padding: 1.25em 1.5em"><code>
<!-- @inproceedings{zhang2024e3gen,
  title={$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation},
  author={Zhang, weitian and Yan, Yichao and Liu, Yunhui and Sheng, Xingdong and Yang, Xiaokang},
  booktitle={Arxiv},
  year={2024}
} -->
@article{zhang2025layeravatar,
    title={Disentangled Clothed Avatar Generation via Layered Representation}, 
    author={Weitian Zhang and Sijing Wu and Manwen Liao and Yichao Yan},
    year={2024},
    journal={arXiv preprint arXiv:2501.04631},
}
        </code></pre>
      </div>
    </section>

    <!-- <section id="paper">
        <h2>Paper</h2>
        <hr/>
        <figure style="width: 100%;">
            <a href="https://arxiv.org/pdf/2304.03950.pdf">
                <img width="100%" src="assets/paper.png">
            </a>
        </figure>
        <hr/>
    </section> -->

</div>
</body>
</html>
